# –∫–∞–∫ –∫–∞–∑–∞–∫–∏ –ø–æ—Ç–µ—Ä—è–Ω–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏—Å–∫–∞–ª–∏

–¢–∏–∫–µ—Ç: https://github.com/tarantool/tarantool/issues/11404

–ü–æ–ø—ã—Ç–∫–∞ –ø–æ—Ñ–∏–∫—Å–∏—Ç—å: https://github.com/tarantool/tarantool/pull/11406

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–ø—É—Å–∫–∞ 1mops_write.lua –Ω–∞ Lenovo:

<table>
  <thead>
    <tr>
      <th></th>
      <th>complete</th>
      <th>submit</th>
      <th>none</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>–ë–µ–∑ limbo –≤–æ—Ä–∫–µ—Ä–∞ </strong></td>
      <td><code>release: 510k rps</code></td>
      <td>unknown</td>
      <td>unknown</td>
    </tr>
    <tr>
      <td><strong>–° limbo –≤–æ—Ä–∫–µ—Ä–æ–º</strong></td>
      <td><code>release: 270k rps</code></td>
      <td>unknown</td>
      <td>unknown</td>
    </tr>
  </tbody>
</table>

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–ø—É—Å–∫–∞ 1mops_write.lua –Ω–∞ ThinkPad:

<table>
  <thead>
    <tr>
      <th></th>
      <th>complete</th>
      <th>submit</th>
      <th>none</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>–ë–µ–∑ limbo –≤–æ—Ä–∫–µ—Ä–∞ </strong></td>
      <td><code>debug: 270k rps</code> <code>release: 600k rps</code></td>
      <td>unknown</td>
      <td>unknown</td>
    </tr>
    <tr>
      <td><strong>–° limbo –≤–æ—Ä–∫–µ—Ä–æ–º</strong></td>
      <td><code>debug: 110k rps</code><code>release: 310k rps</code></td>
      <td><code>debug: 150k rps</code> <code>release: 370k rps</code></td>
      <td><code>debug: 150k rps</code> <code>release: 370k rps</code></td>
    </tr>
  </tbody>
</table>

---

<details>
<summary>–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –í–ª–∞–¥–∞</summary>

Tbh, I don't understand what does this patch optimize in performance exactly - latency, throughput? With the throughput I don't think the problem was that the confirm was synchronous. It was sync yes, but it was written for batches of transactions.

For example, if you write it slow and rare, then on the next write you simply confirm even more txns. If you write it fast and frequent, you will just write more CONFIRMs, putting pressure on the journal. And the oldest entry in the limbo will have to wait the same amount of time anyway, regardless how many confirmations are being queued after the oldest one.

Have you tried you bench with just the `fiber_call()` optimization? I don't think it must have affected the throughput, but seems like that could indeed help the latency.

Have you also tried increasing the limbo max size? I imagine that the bigger it is, the more txns we can confirm at once, which was the strong side of the old implementation.

I mean, this whole place looks like how normal transactions talk to WAL. We don't write each txn into WAL individually hoping that each txn then would be committed faster. Instead, we write them in batches. Writes themselves become longer, but average time per transaction spent for writing becomes quite smaller. I am puzzled why here it was not visible in the benches ...

I can imagine though, that perhaps we are solving a deeper problem with a bazuka here. I imagine in a single-node cluster the benchmark produces a lot of transactions in a single iteration of the event loop. We, I guess, start confirming the first one because it is immediately confirmed. But if we would have waited until the end of the iteration of the event loop, we could catch a bigger confirm LSN. Just a guess. Might be not true.

That makes me think why wasn't it already working like this before (if it wasn't and now it does). Because `fiber_wakeup()` makes the fiber get executed in the end of the event loop, no? It won't be executed right after the current fiber. Which means it should have caught the latest confirm LSN of the current event loop iteration.

I think we need to investigate deeper. Currently we absolutely surely increase the average WAL overhead per sync txn to fight probably a barely related problem, in a very artificial usecase of a single-node cluster with sync replication.
</details>

---

<details>
<summary>–º–æ–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π</summary>

> Have you tried you bench with just the fiber_call() optimization? I don't think it must have affected the throughput, but seems like that could indeed help the latency.

<details>
<summary>experimental diff</summary>

```diff
/** Confirm an LSN in the limbo. */
static void
txn_limbo_confirm_lsn(struct txn_limbo *limbo, int64_t confirm_lsn)
{
	assert(confirm_lsn > limbo->volatile_confirmed_lsn);
	limbo->volatile_confirmed_lsn = confirm_lsn;
+	if (limbo->worker->flags & FIBER_IS_READY) {
+		limbo->worker->flags ^= FIBER_IS_READY;
+		rlist_del_entry(limbo->worker, state);
+	} else {
+		assert(rlist_empty(&limbo->worker->state));
+	}
+	fprintf(stderr, "txn_limbo_confirm_lsn: %ld\n", confirm_lsn);
+	fflush(stderr);
+	fiber_call(limbo->worker);
-	fiber_wakeup(limbo->worker);
}

static inline int
journal_write(struct journal_entry *entry)
{
	if (journal_write_submit(entry) != 0)
		return -1;
	while (!entry->is_complete) {
		fiber_yield();
+		if (!entry->is_complete) {
+			fprintf(stderr, "early awakening\n");
+			struct synchro_request req;
+			struct vclock vclock;
+			xrow_decode_synchro(entry->rows[0], &req, &vclock);
+			fprintf(stderr, "i'm writing %ld\n", req.lsn);
+			fflush(stderr);
+		}
	}
```
</details>

<details>
<summary>output</summary>

```console
tarantool:~$ tarantool perf/lua/1mops_write.lua --nodes=1 --fibers=6000 --ops=1000000 --transaction=1 --warmup=10 --sync
...
txn_limbo_confirm_lsn: 996654
early awakening
i'm writing 990804
txn_limbo_confirm_lsn: 996655
early awakening
i'm writing 990804
txn_limbo_confirm_lsn: 996656
early awakening
i'm writing 990804
txn_limbo_confirm_lsn: 996657
early awakening
i'm writing 990804
txn_limbo_confirm_lsn: 996658
early awakening
i'm writing 990804
txn_limbo_confirm_lsn: 996659
...
```
</details>

Only `fiber_call` doesn't affect rps, if the worker keeps waiting for the write to finish. It just wakes up in vain.

<table border="0">
 <tr>
    <td>`fiber_wakeup`</td>
    <td>`fiber_call`</td>
 </tr>
 <tr>
    <td>

```console
...
# Warmup... done, lsn: 	101837
# master done 894641 ops in time: 6.892991, cpu: 10.146884
# master average speed	129789	ops/sec
# master peak speed	217559	ops/sec
1mops_master 129789 rps
```
</td>
    <td>

```console
...
# Warmup... done, lsn: 	102055
# master done 894596 ops in time: 7.226434, cpu: 10.596610
# master average speed	123794	ops/sec
# master peak speed	242514	ops/sec
1mops_master 123794 rps
```
</td>
 </tr>
</table>

> Have you also tried increasing the limbo max size? I imagine that the bigger it is, the more txns we can confirm at once, which was the strong side of the old implementation.

Now we are talking about the single-instance case, `txn_limbo_confirm_lsn` is called for **each** transaction, i.e. for each transaction we bump `limbo->volatile_confirmed_lsn` by 1 and wake up `limbo->worker`. With my patch, confirm is written for each transaction (batch size = 1), which is of course wrong. I decided to look at the size of this batch before this PR:
<details>
<summary>experimental diff</summary>

```diff
static int
txn_limbo_write_confirm(struct txn_limbo *limbo, int64_t lsn)
{
+	static int64_t prev_lsn = 0;
+	fprintf(stderr, "txn_limbo_write_confirm: %ld\n", lsn - prev_lsn);
+	fflush(stderr);
+	prev_lsn = lsn;
```
</details>

<details>
<summary>output</summary>

```console
tarantool:~$ tarantool perf/lua/1mops_write.lua --nodes=1 --fibers=6000 --ops=1000000 --transaction=1 --warmup=10 --sync
...
txn_limbo_write_confirm: 5798
txn_limbo_write_confirm: 200
txn_limbo_write_confirm: 5
txn_limbo_write_confirm: 5798
txn_limbo_write_confirm: 200
txn_limbo_write_confirm: 5
txn_limbo_write_confirm: 5798
txn_limbo_write_confirm: 200
txn_limbo_write_confirm: 5
txn_limbo_write_confirm: 5798
txn_limbo_write_confirm: 200
txn_limbo_write_confirm: 5
...
```
</details>

Increasing the maximum limbo size does not affect anything in this case, because in this test we have 6000 fibers (6000 transactions at the same time), each transaction is ~61 bytes, so the peak limbo size will be about 366000 bytes, and `replication_synchro_queue_max_size` by default is 16777216. All transactions immediately go to limbo.

I checkouted [017700dccdb04edd8470af620c69f3e7c572895b](the state) when the worker was not there yet (when there was a good result on the benchmark) to make sure that confirm was written for each transaction. And it really is:
<details>
<summary>experimental diff</summary>

```diff
static void
txn_limbo_write_confirm(struct txn_limbo *limbo, int64_t lsn)
{
+	fprintf(stderr, "txn_limbo_write_confirm: %ld\n", lsn);
+	fflush(stderr);
	assert(lsn > limbo->confirmed_lsn);
	assert(!limbo->is_in_rollback);
	limbo->confirmed_lsn = lsn;
	vclock_follow(&limbo->confirmed_vclock, limbo->owner_id, lsn);
	txn_limbo_write_synchro(limbo, IPROTO_RAFT_CONFIRM, lsn, 0, NULL);
}
```
</details>

<details>
<summary>output</summary>

```console
tarantool:~$ tarantool perf/lua/1mops_write.lua --nodes=1 --fibers=6000 --ops=1000000 --transaction=1 --warmup=10 --sync
...
txn_limbo_write_confirm: 1991437
txn_limbo_write_confirm: 1991438
txn_limbo_write_confirm: 1991439
txn_limbo_write_confirm: 1991440
txn_limbo_write_confirm: 1991441
txn_limbo_write_confirm: 1991442
txn_limbo_write_confirm: 1991443
txn_limbo_write_confirm: 1991444
txn_limbo_write_confirm: 1991445
txn_limbo_write_confirm: 1991446
...
```
</details>

It turns out that in this patch I simply reproduced the behavior that was before, when each fiber was responsible for writing confirm to its transaction. But earlier, when there was no worker, some batching still occurred in the case when the cluster consisted of **several (> 1)** instances. This batching was due to the fact that the replicas sent ack not for each transaction, but for some batch:
<details>
<summary>experimental diff</summary>

```diff
static void
txn_limbo_write_confirm(struct txn_limbo *limbo, int64_t lsn)
{
+	static int64_t prev_lsn = 0;
+	fprintf(stderr, "txn_limbo_write_confirm: %ld\n", lsn - prev_lsn);
+	fflush(stderr);
+	prev_lsn = lsn;
	assert(lsn > limbo->confirmed_lsn);
	assert(!limbo->is_in_rollback);
	limbo->confirmed_lsn = lsn;
	vclock_follow(&limbo->confirmed_vclock, limbo->owner_id, lsn);
	txn_limbo_write_synchro(limbo, IPROTO_RAFT_CONFIRM, lsn, 0, NULL);
}
```
</details>

<details>
<summary>output</summary>

```console
tarantool:~$ tarantool perf/lua/1mops_write.lua --nodes=2 --fibers=6000 --ops=1000000 --transaction=1 --warmup=10 --sync
...
txn_limbo_write_confirm: 5964
txn_limbo_write_confirm: 38
txn_limbo_write_confirm: 5964
txn_limbo_write_confirm: 38
txn_limbo_write_confirm: 5964
txn_limbo_write_confirm: 38
txn_limbo_write_confirm: 5964
txn_limbo_write_confirm: 37
txn_limbo_write_confirm: 1
txn_limbo_write_confirm: 5966
txn_limbo_write_confirm: 36
txn_limbo_write_confirm: 2827
...
```
</details>

But in this patch I forced to write confirmation immediately for each transaction without any batching at all. This is, of course, not correct. It needs to be looked into more closely, I agree that it shouldn't be fixed this way.

To summarize, now the worker that works in the master really batches records, but the batch sizes sometimes look strange: 5798, 200, 5, 5798, 200, 5, ... It would be clearer if they were all around 6000.
It's probably worth comparing the time it takes to make one confirm-writing and the time it takes to make one iteration of an event-loop (6000 fibers doing its execution step). Maybe then something will become clearer.

> Because fiber_wakeup() makes the fiber get executed in the end of the event loop, no? It won't be executed right after the current fiber. Which means it should have caught the latest confirm LSN of the current event loop iteration.

Yes, that's certainly true. Moreover, as I understand it, `fiber_wakeup` and the corresponding `fiber_call` (triggered by this `fiber_wakeup`) will be executed in **the same iteration** of the event loop (in the same iteration of `ev_run`-loop).

Let's imagine two fibers waking each other up:
<details>
<summary>experiment</summary>

```cpp
static int
fiber_f(va_list ap)
{
	(void) ap;
	struct fiber *f = (struct fiber *)(fiber()->f_arg);
	while (true) {
		fiber_wakeup(f);
		fiber_yield();
	}
	return 0;
}

f1 = fiber_new("f1", fiber_f);
f2 = fiber_new("f2", fiber_f);
f1->f_arg = f2;
f2->f_arg = f1;
fiber_set_joinable(f1, true);
fiber_set_joinable(f2, true);
fiber_wakeup(f1);
fiber_wakeup(f2);
fiber_join(f1);
fiber_join(f2);
```
</details>
All execution will happen inside `ev_invoke_pending`:
<details>
<summary>ev_invoke_pending</summary>

https://github.com/tarantool/tarantool/blob/526bc5a128f5fe9be7919ec3441aceae362d2414/third_party/libev/ev.c#L3781-L3802
</details>

The event-loop will get stuck in one iteration. No other fibers that went to sleep by calling `fiber_sleep` will receive execution anymore. Just in case, checked in gdb.

Perhaps not particularly relevant to this situation, but still an interesting fact. I thought that `fiber_wakeup` would trigger the fibers to start up on the **next iteration** of the loop.
</details>

---

<details>
<summary>–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –í–ª–∞–¥–∞</summary>

Oh wow, those are cool results ü§Ø! Thanks for checking this all so detailed!

> Only fiber_call doesn't affect rps, if the worker keeps waiting for the write to finish. It just wakes up in vain.

Did I get it right, that `fiber_call()` alone had only slowed down the processing ü§î?

> but the batch sizes sometimes look strange: 5798, 200, 5, 5798, 200, 5, ... It would be clearer if they were all around 6000.

I firstly wanted to explain this via a lot of txns piling up while the previous confirmation is being written to WAL, but then indeed while confirmation is being written, the next batch would always be as big.

The txns in the bench - are they blocking? Or are they `wait_mode = 'submit'`? Because if they are blocking, then it might explain the thing.

Firstly a lot of txns start committing, and confirm goes to WAL to persist their result. While it is being written to WAL, you have only a small part of fibers left which produce new txns. When confirm is done, you send the next batch (small one) to WAL, and the other fibers who were just confirmed by the previous write (a lot of them) produce the next large batch. And this way they exchange somehow.

> It turns out that in this patch I simply reproduced the behavior that was before

Does this mean that the old no-worker-fiber code was also x2 faster on this bench in the single-node case?

> Yes, that's certainly true.

Hm. So this means, that if 1000 fibers call fiber_wakeup on the limbo worker, the worker will be pushed to the end of the event loop iteration. Because each wakeup will move it to the tail of the fiber list via `rlist_move_tail_entry(&cord->ready, f, state);` in `fiber_make_ready()`.

I am curious what would happen if we make the following: only call `fiber_wakeup()` where there is a reason to wakeup? Like that the volatile confirmed LSN > persisted confirmed LSN. Then we would make less of these `rlist_move_tail_entry()` calls and the closest confirm would be written sooner. Assuming that between those wakeups some other fibers were standing into the end and then we again were pushing the limbo worker to the end.

**Update**: wake it up not when volatile LSN > persisted LSN (it is always the case), but rather when the fiber just isn't already woken up. So it doesn't have `FIBER_IS_READY` flag. Or a less "intrusive way" would be have a flag in the limbo like "has work". When it is false, we wake the fiber up and set the flag. When it is true, we do nothing. And the worker itself will remove the flag before going to sleep in `txn_limbo_worker_f()`.

We then also get rid of the spurious wakeups of its journal write of the currently being written confirmation entry.
</details>

---

### –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ 3000 —Ñ–∞–π–±–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 6000 –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ–¥—Å–∫–æ—á–∏–ª–∞ –Ω–∞ 70%:
```console
~/dev/tarantool/release-build/src/tarantool ~/dev/tarantool/perf/lua/1mops_write.lua --nodes=1 --fibers=3000 --ops=1000000 --transaction=1 --warmup=10 --sync
# making 1000000 REPLACE operations,
# 1 operations per txn,
# using 3000 fibers,
# in a replicaset of 1 nodes,
# using HASH index type
# with WAL mode write
# 
# promoting
# done
# Warmup... done, lsn: 	101833
# master done 898182 ops in time: 1.683649, cpu: 2.620826
# master average speed	533473	ops/sec
# master peak speed	593731	ops/sec
1mops_master 533473 rps
```

![alt text](plotting-fibers-rps/plot.jpg "1mops")

–°—Ç—Ä–∞–Ω–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å rps –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–±–µ—Ä–æ–≤. –ü—Ä–∏–º–µ—Ä–Ω–æ –¥–æ 4500 —Ä–∞–∑–±—Ä–æ—Å –∫–∞–∫–æ–π-—Ç–æ –∞–Ω–æ–º–∞–ª—å–Ω–æ –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π. –•–æ—Ä–æ—à–æ –±—ã –∫–∞–∫-—Ç–æ –ø–æ–Ω—è—Ç—å, —á—Ç–æ –º–µ–Ω—è–µ—Ç—Å—è –ø—Ä–∏ > 4500 —Ñ–∞–π–±–µ—Ä–æ–≤.

### CPU usage from htop:

–±–µ–∑ –≤–æ–∫–µ—Ä–∞:
![alt text](good/htop.png "htop")

c –≤–æ–∫–µ—Ä–æ–º:
![alt text](bad/htop.png "htop")

–ö–∞–∫–æ–π-—Ç–æ –∑–Ω–∞—á–∏–º–æ–π —Ä–∞–∑–Ω–∏—Ü—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ. –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç—å WAL –≤ –æ–±–æ–∏—Ö —Å–ª—É—á–∞—è—Ö 45-55%. Tx –≤ –æ–±–æ–∏—Ö —Å–ª—É—á–∞—è—Ö –∑–∞–≥—Ä—É–∂–µ–Ω –≤ 100%.
–ï—Å–ª–∏ TX –∑–∞–≥—Ä—É–∂–µ–Ω –≤ 100%, –Ω–µ —Å–æ–≤—Å–µ–º –ø–æ–Ω—è—Ç–Ω–æ, –æ—Ç–∫—É–¥–∞ –≤–æ–æ–±—â–µ –º–æ–≥–ª–æ –≤–∑—è—Ç—å—Å—è –∫–∞–∫–æ–µ-—Ç–æ —Ä–∞–∑–ª–∏—á–∏–µ –≤ rps.

### –ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –æ–¥–∏–Ω —Ñ–∞–π–±–µ—Ä (`--fibers=1` ThinkPad):

<table>
  <thead>
    <tr>
      <th>complete</th>
      <th>submit</th>
      <th>none</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>release: 94k rps</code></td>
      <td><code>release: 540-610k rps</code></td>
      <td><code>release: 610k rps</code></td>
    </tr>
  </tbody>
</table>

![alt text](1fiber_htop.png "htop")

–û–¥–∏–Ω —Ñ–∞–π–±–µ—Ä —Å wait='complete' –∑–∞–≥—Ä—É–∂–∞–µ—Ç Tx –≤ 45%. –í 100% Tx –Ω–∞—á–∏–Ω–∞–µ—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ 10 —Ñ–∞–π–±–µ—Ä–æ–≤. –ò –∫–∞–∫ —Ä–∞–∑ –ø—Ä–∏–º–µ—Ä–Ω–æ –Ω–∞ —ç—Ç–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö (10-15 —Ñ–∞–π–±–µ—Ä–æ–≤) –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.

<details>
<summary>–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ —Ç–æ, –∫–∞–∫ –ø–ª–∞–Ω–∏—Ä—É—é—Ç—Å—è —Ñ–∞–π–±–µ—Ä—ã:</summary>

```
transfer from: 1mops_write.lua to sched


schedule 15 fibers
transfer from: sched to loader1
transfer from: loader1 to loader2
transfer from: loader2 to loader3
transfer from: loader3 to loader4
transfer from: loader4 to loader5
transfer from: loader5 to loader6
transfer from: loader6 to loader7
transfer from: loader7 to loader8
transfer from: loader8 to loader9
transfer from: loader9 to loader10
transfer from: loader10 to loader11
transfer from: loader11 to loader12
transfer from: loader12 to loader13
transfer from: loader13 to loader14
transfer from: loader14 to loader15
transfer from: loader15 to sched

## –ó–¥–µ—Å—å —Ñ–∞–π–±–µ—Ä 1mops_write.lua –∑–∞–≤–µ–ª 15 —Ñ–∞–π–±–µ—Ä–æ–≤, –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª –∫–∞–∂–¥—ã–π, –∏ –≤–æ—Ç –æ–Ω–∏ –≤—Å–µ –ø–æ –ø–æ—Ä—è–¥–∫—É –ø–æ–∏—Å–ø–æ–ª–Ω—è–ª–∏—Å—å (—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –¥–æ —Ü–∏–∫–ª–∞ –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏). –ò—Å–ø–æ–ª–Ω–µ–Ω–∏–µ –Ω–∞—á–∞–ª–æ—Å—å —Å –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ –∏ –≤–µ—Ä–Ω—É–ª–æ—Å—å –æ–±—Ä–∞—Ç–Ω–æ –≤ –Ω–µ–≥–æ.

schedule 1 fibers
transfer from: sched to lua
transfer from: lua to sched
schedule 1 fibers
transfer from: sched to 1mops_write.lua
transfer from: 1mops_write.lua to sched

## –ó–¥–µ—Å—å sched –∏—Å–ø–æ–ª–Ω—è–µ—Ç cbus_process –∏ –≤—ã–∑—ã–≤–∞–µ—Ç txn_on_journal_write –¥–ª—è –∫–∞–∂–¥–æ–π –∏–∑ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π (–æ–Ω–∏ –≤—Å–µ —É–∂–µ —É—Å–ø–µ–ª–∏ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è). –ò–∑ txn_on_journal_write –æ–Ω –±—É–¥–∏—Ç –∫–∞–∂–¥–æ–≥–æ –∏–∑ loader'–æ–≤, –∞ —Ç–∞–∫–∂–µ –≤—ã–∑—ã–≤–∞–µ—Ç txn_limbo_ack, –æ—Ç–∫—É–¥–∞ –±—É–¥–∏—Ç –≤–æ—Ä–∫–µ—Ä–∞:

## #2  0x627cb25e3499 in txn_on_journal_write+473
## fiber sched wakes up fiber loader1
## #2  0x627cb25e36c0 in txn_on_journal_write+1024
## fiber sched wakes up fiber txn_limbo_worker
## #2  0x627cb25e3499 in txn_on_journal_write+473
## fiber sched wakes up fiber loader2
## #2  0x627cb25e36c0 in txn_on_journal_write+1024
## fiber sched wakes up fiber txn_limbo_worker
## #2  0x627cb25e3499 in txn_on_journal_write+473
## fiber sched wakes up fiber loader3
## #2  0x627cb25e36c0 in txn_on_journal_write+1024
## fiber sched wakes up fiber txn_limbo_worker
## #2  0x627cb25e3499 in txn_on_journal_write+473
## fiber sched wakes up fiber loader4
## #2  0x627cb25e36c0 in txn_on_journal_write+1024

schedule 16 fibers
transfer from: sched to loader1
transfer from: loader1 to txn_limbo_worker

## –ü–æ–Ω—è—Ç–Ω–æ, –ø–æ—á–µ–º—É txn_limbo_worker –ø–æ–ø–∞–ª –ø–æ—Å–ª–µ loader1, –Ω–æ –ø–µ—Ä–µ–¥ loader2 –∏ –¥—Ä—É–≥–∏–º–∏. txn_on_journal_write –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ —Å–ø–∏—Å–æ–∫ loader1 –∏ —Å–ª–µ–¥–æ–º –∑–∞ –Ω–∏–º txn_limbo_worker, –∞ –≤—Å–µ –¥–∞–ª—å–Ω–µ–π—à–∏–µ –ø–æ–ø—ã—Ç–∫–∏ —Ä–∞–∑–±—É–¥–∏—Ç—å txn_limbo_worker –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞—é—Ç (–Ω–µ –ø–µ—Ä–µ–∫–ª–∞–¥—ã–≤–∞—é—Ç –µ–≥–æ –≤ –∫–æ–Ω–µ—Ü), –ø–æ—Ç–æ–º—É —á—Ç–æ –µ—Å—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∞:

## const int no_flags = FIBER_IS_READY | FIBER_IS_DEAD | ## FIBER_IS_RUNNING;
## 	if ((f->flags & no_flags) == 0)
## 		fiber_make_ready(f);

–ó–¥–µ—Å—å –º—ã –∑–∞–ø—É—à–∏–ª–∏ –≤ wal –∑–∞–ø–∏—Å—å COMFIRM –Ω–∞ lsn ~= 23.

transfer from: txn_limbo_worker to loader2
transfer from: loader2 to loader3
transfer from: loader3 to loader4
transfer from: loader4 to loader5
transfer from: loader5 to loader6
transfer from: loader6 to loader7
transfer from: loader7 to loader8
transfer from: loader8 to loader9
transfer from: loader9 to loader10
transfer from: loader10 to loader11
transfer from: loader11 to loader12
transfer from: loader12 to loader13
transfer from: loader13 to loader14
transfer from: loader14 to loader15
transfer from: loader15 to sched

–ó–¥–µ—Å—å –≤—Å–µ —Ñ–∞–π–±–µ—Ä—ã –≤—ã—à–ª–∏ –∏–∑ –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–ø–∏—Å–∏ –∏ –ø–µ—Ä–µ—à–ª–∏ –≤ –æ–∂–∏–¥–∞–Ω–∏–µ CONFIRM (txn_limbo_wait_complete).

## –ó–¥–µ—Å—å —É–∂–µ –Ω–µ —Ç–∞–∫ –ø–æ–Ω—è—Ç–Ω–æ, –ø–æ—á–µ–º—É –æ–Ω–∏ –≤—Å–µ –ø–æ –ø–æ—Ä—è–¥–æ—á–∫—É –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–∏—Å—å.
## –ü–æ—ç—Ç–æ–º—É —è –¥–æ–±–∞–≤–∏–ª –ø—Ä–∏–Ω—Ç—ã –≤–∏–¥–∞:
## #2  0x5e3154714499 in txn_on_journal_write+473
## fiber sched wakes up fiber loader1

## –ò –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –∏—Ö –≤—Å–µ—Ö –≤–æ—Ç —Ç–∞–∫ –ø–æ –ø–æ—Ä—è–¥–æ—á–∫—É —Ä–∞–∑–±—É–¥–∏–ª `txn_on_journal_write`. –¢–æ –µ—Å—Ç—å, –∫–∞–∫ –º—ã –∏ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–ª–∏, —Ñ–∞–π–±–µ—Ä—ã –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–∏—Å—å –Ω–∞ –æ–∂–∏–¥–∞–Ω–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏. –ò –≤–æ—Ç sched –ø—Ä–æ–∫—Ä—É—Ç–∏–ª –∫–æ–ª–±–µ–∫–∏ –Ω–∞ —ç—Ç–∏ –∑–∞–ø–∏—Å–∏.

## static struct cmsg_hop wal_request_route[] = {
## 	{wal_write_to_disk, &wal_writer_singleton.tx_prio_pipe},
## 	{tx_complete_batch, NULL},
## };

## ev_run -> ev_invoke_pending -> tx_prio_cb -> cbus_process -> cmsg_deliver -> tx_complete_batch -> txn_on_journal_write

## –ü–æ—á–µ–º—É tx_prio –∏–º–µ–µ—Ç —Å—É—Ñ—Ñ–∏–∫—Å prio? –í —á–µ–º –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç—å? –ê –≤ —Ç–æ–º, –æ—Ç–∫—É–¥–∞ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è cbus_process. –í –Ω–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–º —Å–ª—É—á–∞–µ —Ç–∏–ø–∞ wal_worker, cbus_process –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –∏–∑ cbus_loop. –¢–æ –µ—Å—Ç—å cbus_loop –∫—Ä—É—Ç–∏—Ç—Å—è –≤ wal_worker –∏ –∏–ª–¥–∏—Ç –≤ –æ–∂–∏–¥–∞–Ω–∏–∏ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ—Ä—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. cpipe_flush –¥–µ–ª–∞–µ—Ç ev_async_send –≤ endpoint->async. –í —Å–ª—É—á–∞–µ —Å wal_worker –Ω–∞ —ç—Ç–æ–º —Å–æ–±—ã—Ç–∏–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∫–æ–ª–±–µ–∫ fiber_schedule_cb, –∫–æ—Ç–æ—Ä—ã–π –∫–∞–∫ —Ä–∞–∑ –±—É–¥–∏—Ç —Ñ–∞–π–±–µ—Ä, –≤ –∫–æ—Ç–æ—Ä–æ–º –∫—Ä—É—Ç–∏—Ç—Å—è cbus_loop, —á—Ç–æ–±—ã –æ–Ω –ø–æ—à–µ–ª –∏ –ø–æ–¥—Ö–≤–∞—Ç–∏–ª –æ—á–µ—Ä–µ–¥–Ω—É—é –ø–æ—Ä—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –∏–∑ endpoint->output.
–í —Å–ª—É—á–∞–µ —Å tx_prio, –Ω–∏–∫–∞–∫–æ–π cbus_loop –Ω–µ –∫—Ä—É—Ç–∏—Ç—Å—è –≤ —Ñ–∞–π–±–µ—Ä–µ, —É –Ω–µ–≥–æ –ø—Ä–æ—Å—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥—Ä—É–≥–æ–π –∫–æ–ª–±–µ–∫ –Ω–∞ endpoint->async - tx_prio_cb, –∫–æ—Ç–æ—Ä—ã–π —Ç—É—Ç –∂–µ –∑–æ–≤–µ—Ç cbus_process. –¢–æ –µ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–π–ø–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å—Ä–∞–∑—É –∂–µ.

–í –æ–¥–Ω–æ–º —Å–ª—É—á–∞–µ ev_async_send ----> ev_invoke_pending -> tx_prio_cb –∏ –∑–¥–µ—Å—å –∂–µ —Å—Ä–∞–∑—É –≤—ã–∑—ã–≤–∞—é—Ç—Å—è –∫–æ–ª–±–µ–∫–∏ –Ω–∞ –ø—Ä–∏—à–µ–¥—à–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è.

–í –¥—Ä—É–≥–æ–º —Å–ª—É—á–∞–µ ev_async_send ----> ev_invoke_pending -> fiber_schedule_cb -> fiber_wakeup –∑–∞—Ç–µ–º –æ—Ç—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ü–µ–ø–æ—á–∫–∞ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–±–µ—Ä–æ–≤ –∏ –≤ –∫–æ–Ω—Ü–µ –æ—á–µ—Ä–µ–¥—å –¥–æ—Ö–æ–¥–∏—Ç –¥–æ –Ω–∞—à–µ–≥–æ wal_worker, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—ã–∑—ã–≤–∞–µ—Ç –∫–æ–ª–±–µ–∫–∏ –Ω–∞ –ø—Ä–∏—à–µ–¥—à–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è.

–ì—Ä—É–±–æ –≥–æ–≤–æ—Ä—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π endpoint –Ω–µ –∂–¥–µ—Ç —Å–≤–æ–µ–π –æ—á–µ—Ä–µ–¥–∏, –ø–æ–∫–∞ –æ—Ç—Ä–∞–±–æ—Ç–∞–µ—Ç –∫—É—á–∞ –¥—Ä—É–≥–∏—Ö —Ñ–∞–π–±–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∫ —ç—Ç–æ–º—É –º–æ–º–µ–Ω—Ç—É —É—Å–ø–µ–ª–∏ –Ω–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å—Å—è (–≤—Å—Ç–∞—Ç—å –≤ ready), –∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–≤–æ–∏ –∫–æ–ª–±–µ–∫–∏ –ø–µ—Ä–≤—ã–º.



–ó–¥–µ—Å—å –ø—Ä–æ–∏–∑–æ—à–µ–ª –≤—ã–∑–æ–≤ –∫–æ–ª–±–µ–∫–∞ tx_complete_batch –Ω–∞ –∑–∞–ø–∏—Å—å CONFIRM, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª —Ä–∞–∑–±—É–¥–∏—Ç—å worker –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ —Ç—É—Ç –∂–µ –±—É–¥–∏—Ç –µ–≥–æ. –ù–æ –∑–¥–µ—Å—å worker –Ω–∏–∫–∞–∫—É—é –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å –Ω–µ –ø–æ—Ä–æ–∂–¥–∞–µ—Ç, –ø–æ—Ç–æ–º—É —á—Ç–æ CONFIRM —É–∂–µ –∑–∞–ø–∏—Å–∞–Ω –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é (–Ω–∞ –ø–µ—Ä–≤—ã–π –±–ª–æ–∫ –∏–∑ 15 —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π).



schedule 1 fibers
transfer from: sched to txn_limbo_worker
transfer from: txn_limbo_worker to sched

–ù–æ –∑–¥–µ—Å—å –≤–æ—Ä–∫–µ—Ä –≤—ã–∑—ã–≤–∞–µ—Ç txn_limbo_read_confirm, –æ—Ç–∫—É–¥–∞ –æ–Ω –≤—ã–∑—ã–≤–∞–µ—Ç fiber_wakeup –Ω–∞ –∫–∞–∂–¥—ã–π –∏–∑ loader'–æ–≤. –°–µ–π—á–∞—Å –≤—Å–µ 15 –ª–æ–∞–¥–µ—Ä–æ–≤ –ø–æ–∑–æ–≤—É—Ç yield.

schedule 15 fibers
transfer from: sched to loader1
yield
transfer from: loader1 to loader2
yield
transfer from: loader2 to loader3
yield
transfer from: loader3 to loader4
yield
transfer from: loader4 to loader5
yield
transfer from: loader5 to loader6
yield
transfer from: loader6 to loader7
yield
transfer from: loader7 to loader8
yield
transfer from: loader8 to loader9
yield
transfer from: loader9 to loader10
yield
transfer from: loader10 to loader11
yield
transfer from: loader11 to loader12
yield
transfer from: loader12 to loader13
yield
transfer from: loader13 to loader14
yield
transfer from: loader14 to loader15
yield
transfer from: loader15 to sched

## –ê –∑–¥–µ—Å—å —Ñ–∞–π–±–µ—Ä—ã –ø–æ—á–µ–º—É-—Ç–æ –Ω–∞—á–∏–Ω–∞—é—Ç –±—É–¥–∏—Ç—å—Å—è –∏–∑ fiber_schedule_timeout. –í—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫ –±—É–¥—Ç–æ –±—ã –º—ã –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏–∑ —Ñ–∞–π–±–µ—Ä–æ–≤ –ø–æ–∑–≤–∞–ª–∏ fiber_sleep, —Ö–æ—Ç—è –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –º—ã –ø–æ–∑–≤–∞–ª–∏ yield.

–ù–µ –ø–æ–Ω—è—Ç–Ω–æ –ø–æ—á–µ–º—É –≤ –ª–æ–∞–¥–µ—Ä–µ –º—ã –≤—ã–∑—ã–≤–∞–µ–º yield –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, –∏ –Ω–µ –ø–æ–Ω—è—Ç–Ω–æ, –∫—Ç–æ –∏—Ö –≤–æ–æ–±—â–µ –±—É–¥–∏—Ç –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ yield:

## local function fiber_load(start, s)
##     start = start % 1000000 -- limit the size of space to 1M ## elements
##     for _ = 1, trans_per_fiber do
##         box.begin()
##         for _ = 1, ops_per_txn do
##             s:replace{start}
##             start = start + 1
##         end
##         box.commit({wait='complete'})
##         fiber.yield()
##     end
## end

–û–ö–ê–ó–´–í–ê–ï–¢–°–Ø lbox_fiber_yield = fiber_sleep(0);

–ó–¥–µ—Å—å –ø—Ä–æ–∏–∑–æ—à–ª–æ ev_invoke_pending -> fiber_schedule_timeout -> fiber_wakeup(loader1)

schedule 1 fibers
transfer from: sched to loader1
transfer from: loader1 to sched

ev_invoke_pending -> fiber_schedule_timeout -> fiber_wakeup(loader15)

schedule 1 fibers
transfer from: sched to loader15
transfer from: loader15 to sched

ev_invoke_pending -> fiber_schedule_timeout -> fiber_wakeup(loader13)

schedule 1 fibers
transfer from: sched to loader13
transfer from: loader13 to sched

ev_invoke_pending -> fiber_schedule_timeout -> fiber_wakeup(loader12)

schedule 1 fibers
transfer from: sched to loader12
transfer from: loader12 to sched
schedule 1 fibers
transfer from: sched to loader11
transfer from: loader11 to sched
schedule 1 fibers
transfer from: sched to loader4
transfer from: loader4 to sched
schedule 1 fibers
transfer from: sched to loader9
transfer from: loader9 to sched
schedule 1 fibers
transfer from: sched to loader8
transfer from: loader8 to sched
schedule 1 fibers
transfer from: sched to loader7
transfer from: loader7 to sched
schedule 1 fibers
transfer from: sched to loader14
transfer from: loader14 to sched
schedule 1 fibers
transfer from: sched to loader5
transfer from: loader5 to sched
schedule 1 fibers
transfer from: sched to loader6
transfer from: loader6 to sched
schedule 1 fibers
transfer from: sched to loader10
transfer from: loader10 to sched
schedule 1 fibers
transfer from: sched to loader2
transfer from: loader2 to sched
schedule 1 fibers
transfer from: sched to loader3
transfer from: loader3 to sched

## –°—Ç—Ä–∞–Ω–Ω–æ, —á—Ç–æ –∫–∞–∂–¥–æ–µ –ø—Ä–æ–±—É–∂–¥–µ–Ω–∏–µ –ø–æ–ø–∞–ª–æ –≤ —Å–≤–æ—é –∏—Ç–µ—Ä–∞—Ü–∏—é —Ü–∏–∫–ª–∞.
## –ó–¥–µ—Å—å –ø–æ—Ç–µ—Ä—è–ª–∏—Å—å –¥–≤–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ñ–∞–π–±–µ—Ä–∞ - 2 –∏ 3. –ò—Ö –æ—Ç—Ä–µ–∑–∞–ª–æ, –ø–æ—Ç–æ–º—É —á—Ç–æ WAL –µ—â–µ –Ω–µ —É—Å–ø–µ–ª –∑–∞–ø–∏—Å–∞—Ç—å —ç—Ç–∏ –¥–≤–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏. –û—Ç—Å—é–¥–∞ –∏ –±–µ—Ä–µ—Ç—Å—è —ç—Ç–∞ —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏—è: sleep'—ã –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—é—Ç —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏, –∑–∞–ø–∏—Å—å –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –æ—Ç—Ä–µ–∑–∞–µ—Ç –∫–∞–∫–æ–π-—Ç–æ —Å—É—Ñ—Ñ–∏–∫—Å (–±—É–¥–∏—Ç –Ω–µ –≤—Å–µ 15 –≤–æ—Ä–∫–µ—Ä–æ–≤, –∞ –ª–∏—à—å –∫–∞–∫–æ–π-—Ç–æ –ø—Ä–µ—Ñ–∏–∫—Å, –∫–æ—Ç–æ—Ä—ã–π —É—Å–ø–µ–ª –∑–∞–ø–∏—Å–∞—Ç—å—Å—è).

schedule 14 fibers
transfer from: sched to loader1
transfer from: loader1 to txn_limbo_worker
transfer from: txn_limbo_worker to loader15
transfer from: loader15 to loader13
transfer from: loader13 to loader12
transfer from: loader12 to loader11
transfer from: loader11 to loader4
transfer from: loader4 to loader9
transfer from: loader9 to loader8
transfer from: loader8 to loader7
transfer from: loader7 to loader14
transfer from: loader14 to loader5
transfer from: loader5 to loader6
transfer from: loader6 to loader10
transfer from: loader10 to sched
```
</details>

# Cringe

–ü—Ä–æ–±–ª–µ–º–∞ –±—ã–ª–∞ –≤ —Ç–æ–º, —á—Ç–æ –∑–∞–ø–∏—Å—å CONFIRM —Å—á–∏—Ç–∞–ª–∞—Å—å –∑–∞ –æ–ø–µ—Ä–∞—Ü–∏—é, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ –±–∞–º–ø–∞–µ—Ç lsn. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, —á–µ–º –±–æ–ª—å—à–µ CONFIRM'–æ–≤ –∑–∞–ø–∏—Å–∞–ª, —Ç–µ–º –±–æ–ª—å—à–µ rps. –°–∞–º–æ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ, —á—Ç–æ —è –∑–Ω–∞–ª –æ–± —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥—Ä—É–≥–æ–≥–æ —Ç–∏–∫–µ—Ç–∞, –ø–æ—è–≤–∏–≤—à–µ–≥–æ—Å—è –ø—Ä—è–º–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å —ç—Ç–∏–º, –∏ —É –º–µ–Ω—è –¥–∞–∂–µ —É–∂–µ –±—ã–ª [—Ñ–∏–∫—Å-–ø–∞—Ç—á](https://github.com/tarantool/tarantool/pull/11415) –≤ –º–æ–º–µ–Ω—Ç –≤—Å–µ–≥–æ —ç—Ç–æ–≥–æ —Ä–∞–∑–±–∏—Ä–∞—Ç–µ–ª—å—Å—Ç–≤–∞. –ù–æ –ø—Ä–∏ —ç—Ç–æ–º —è –ø–æ—á–µ–º—É-—Ç–æ –¥–∞–∂–µ –Ω–µ –ø–æ–¥—É–º–∞–ª, —á—Ç–æ —ç—Ç–∞ —Ö–µ—Ä–Ω—è –º–æ–∂–µ—Ç –∞—Ñ—Ñ–µ–∫—Ç–∏—Ç—å –∏ —ç—Ç—É —Å–∏—Ç—É–∞—Ü–∏—é –≤ —Ç–æ–º —á–∏—Å–ª–µ.

–ò –≤ –ø—Ä–∏–Ω—Ü–∏–ø–µ –ø–æ–Ω—è—Ç–Ω–æ, –ø–æ—á–µ–º—É —Å —É–º–µ–Ω—å—à–µ–Ω–∏–µ–º –∫–æ–ª-–≤–∞ —Ñ–∞–π–±–µ—Ä–æ–≤, —Ä–æ—Å–ª–∏ rps - —á–∞—â–µ –ø–∏—Å–∞–ª–∏—Å—å CONFIRM'—ã. –•–æ—Ç—è –≤—Å–µ –µ—â–µ –Ω–µ –ø–æ–Ω—è—Ç–Ω–æ, –ø–æ—á–µ–º—É —Ç–∞–º –Ω–∞—Å—Ç–æ–ª—å–∫–æ –±–æ–ª—å—à–æ–π —Ä–∞–∑–±—Ä–æ—Å –≤–æ–∑–Ω–∏–∫–∞–ª –ø—Ä–∏ —ç—Ç–æ–º –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ.
